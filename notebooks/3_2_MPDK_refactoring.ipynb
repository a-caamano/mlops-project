{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "oh0PDOdoW0gV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier"
      ],
      "metadata": {
        "id": "Hi89j6VLmSGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading and exploring the data"
      ],
      "metadata": {
        "id": "cjR0ZcwZX_at"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0Cqe4qcapQW",
        "outputId": "8c9f1428-2b96-45ec-b6d2-765b6cbf0bbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "def load_data(filepath):\n",
        "  data = pd.read_csv(filepath)\n",
        "  return data\n",
        "\n",
        "def explore_data(data):\n",
        "  print(data.head())\n",
        "  print(data.shape)\n",
        "  print(data.describe().T)\n",
        "  print(data.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EDA"
      ],
      "metadata": {
        "id": "OLbg5u-dboN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analizyng_nulls (data):\n",
        "  nulos = data.isnull().mean()*100\n",
        "  display(nulos.sort_values(ascending=False))\n",
        "\n",
        "  porcentaje_nulos_por_fila = data.isnull().mean(axis=1) * 100\n",
        "  porcentajes_filtrados = porcentaje_nulos_por_fila[porcentaje_nulos_por_fila >= 5]\n",
        "  orden_porcentaje_nulos = porcentajes_filtrados.sort_values(ascending=False)\n",
        "\n",
        "  print(\"\\nNúmero de filas con más del 5% de nulos:\", len(orden_porcentaje_nulos))\n",
        "  display(orden_porcentaje_nulos)\n"
      ],
      "metadata": {
        "id": "rnuK2JGZbr2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyzing_outliers(data):\n",
        "  plt.figure(figsize=(15, 8))\n",
        "  data.boxplot()\n",
        "  plt.title('Boxplot de todas las variables')\n",
        "  plt.xticks(rotation=45)\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "UmYAUvXfc6_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualizing data"
      ],
      "metadata": {
        "id": "9HVBamhTdX0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_data(data):\n",
        "  cols_per_row = 4\n",
        "  total_cols = len(data.columns)\n",
        "  rows = (total_cols + cols_per_row - 1) // cols_per_row\n",
        "\n",
        "  fig, axes = plt.subplots(rows, cols_per_row, figsize=(cols_per_row * 4, rows * 3))\n",
        "  axes = axes.flatten()\n",
        "\n",
        "  for i, col in enumerate(data.columns):\n",
        "    sns.histplot(data[col], kde=True, bins=30, color='skyblue', ax=axes[i])\n",
        "    axes[i].set_title(f'Distribución de: {col}')\n",
        "    axes[i].set_xlabel('')\n",
        "    axes[i].set_ylabel('')\n",
        "\n",
        "  for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "1qSMASV3el25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correlation_analysis(data):\n",
        "  plt.figure()\n",
        "  sns.heatmap(data.corr(), annot=False, cmap='coolwarm', linewidths=0.5, cbar_kws={'shrink': 0.5})\n",
        "  plt.title('Matriz de Correlaciones')\n",
        "  plt.xticks(rotation=90, fontsize=8)\n",
        "  plt.yticks(rotation=0, fontsize=8)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "0sBKXthV6dna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def target_analysis(data):\n",
        "  plt.figure(figsize=(15, 8))\n",
        "  for column in data.columns[:-1]:\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.boxplot(x=data.iloc[:,-1], y=column, data=data)\n",
        "    plt.title(f'Correlación entre la variable objetivo y {column}')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "nTSnNpSZ68r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing and feature engineering"
      ],
      "metadata": {
        "id": "dsP0wxEClUrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def manage_duplicates(data):\n",
        "  data.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "FeOhg-_e7tj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def manage_nulls(data):\n",
        "  porcentaje_nulos_por_fila = data.isnull().mean(axis=1)*100\n",
        "  data = data[porcentaje_nulos_por_fila <= 5]"
      ],
      "metadata": {
        "id": "uLIi0bHO708T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(data):\n",
        "  X = data.iloc[:, :-1]\n",
        "  y = data.iloc[:, -1]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "  return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "ZMces90E7-xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def impute_nulls(X_train):\n",
        "  for col in X_train.columns:\n",
        "    mediana = X_train[col].median()\n",
        "    X_train[col] = X_train[col].fillna(mediana)\n",
        "  return X_train"
      ],
      "metadata": {
        "id": "AVE5uY3K9XMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def impute_outliers(X_train):\n",
        "  for col in X_train.columns:\n",
        "    Q1 = X_train[col].quantile(0.25)\n",
        "    Q3 = X_train[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    limite_inferior = Q1 - 1.5 * IQR\n",
        "    limite_superior = Q3 + 1.5 * IQR\n",
        "    mediana = X_train[col].median()\n",
        "\n",
        "    X_train.loc[(X_train[col] < limite_inferior) | (X_train[col] > limite_superior), col] = mediana\n",
        "  return X_train\n"
      ],
      "metadata": {
        "id": "rdRnQ2pM8l-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def manage_corr(X_train):\n",
        "  correlation_matrix = X_train.corr()\n",
        "  upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
        "  high_corr_columns = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.75)]\n",
        "\n",
        "  X_train = X_train.drop(columns=high_corr_columns)\n",
        "  return X_train"
      ],
      "metadata": {
        "id": "B1CdVaZh-Qrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_features(X_train):\n",
        " scaler = StandardScaler()\n",
        " X_train_scaled = scaler.fit_transform(X_train)\n",
        " return X_train_scaled"
      ],
      "metadata": {
        "id": "A-aAOAx5-uS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pca_analysis(X_train_scaled):\n",
        "  pca = PCA()\n",
        "  X_train_PCA = pca.fit_transform(X_train_scaled)\n",
        "  cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "  n_components_90 = np.argmax(cumulative_variance >= 0.90)+1\n",
        "  pca = PCA(n_components=n_components_90)\n",
        "  X_train_VF = pca.fit_transform(X_train_scaled)\n",
        "  return X_train_VF"
      ],
      "metadata": {
        "id": "S782kdmR_GpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Output variable analysis"
      ],
      "metadata": {
        "id": "tlSLzpnr8StT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def target(y_train):\n",
        "  print(f\"Distribución de clases: {y_train.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'}\")\n",
        "\n",
        "  df = pd.concat([pd.DataFrame(X_train_VF), pd.Series(y_train, name='target')], axis=1)\n",
        "  corr = df.corr()['target'].sort_values(ascending=False)\n",
        "  print(\"Correlación con la variable dependiente:\\n\", corr)"
      ],
      "metadata": {
        "id": "2kkiSxqPAL2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing the test set using training set statistics"
      ],
      "metadata": {
        "id": "mazeFDv9BHy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_test_data(X_train, X_test, high_corr_columns, scaler, pca):\n",
        "  \"\"\"\n",
        "  Preprocesa el conjunto de prueba usando estadísticas del conjunto de entrenamiento.\n",
        "\n",
        "  Pasos:\n",
        "  1. Calcula mediana, límites de outliers (IQR) por columna en X_train.\n",
        "  2. Imputa valores nulos en X_test con la mediana.\n",
        "  3. Sustituye outliers en X_test por la mediana.\n",
        "  4. Elimina columnas altamente correlacionadas.\n",
        "  5. Escala los datos y aplica PCA.\n",
        "\n",
        "  Parámetros:\n",
        "  - X_train: DataFrame de entrenamiento.\n",
        "  - X_test: DataFrame de prueba.\n",
        "  - high_corr_columns: lista de columnas a eliminar.\n",
        "  - scaler: objeto de escalado (ya ajustado).\n",
        "  - pca: objeto PCA (ya ajustado).\n",
        "\n",
        "  Retorna:\n",
        "  - X_test_VF: conjunto de prueba transformado por PCA.\n",
        "  \"\"\"\n",
        "  for col in X_train.columns:\n",
        "    mediana = X_train[col].median()\n",
        "    Q1 = X_train[col].quantile(0.25)\n",
        "    Q3 = X_train[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    limite_inferior = Q1 - 1.5 * IQR\n",
        "    limite_superior = Q3 + 1.5 * IQR\n",
        "\n",
        "    if col in X_test.columns:\n",
        "      X_test[col] = X_test[col].fillna(mediana)\n",
        "      X_test[col] = X_test[col].apply(lambda x: mediana if x < limite_inferior or x > limite_superior else x)\n",
        "\n",
        "  X_test = X_test.drop(columns=high_corr_columns, errors='ignore')\n",
        "\n",
        "  X_test_scaled = scaler.transform(X_test)\n",
        "  X_test_VF = pca.transform(X_test_scaled)\n",
        "\n",
        "  return X_test_VF"
      ],
      "metadata": {
        "id": "B0YK1SZ9BUKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training and evaluating models"
      ],
      "metadata": {
        "id": "IhRyReb8BzkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_model(model, X_train_VF, y_train, X_test_VF, y_test):\n",
        "  \"\"\"\n",
        "  Entrena un modelo de Regresión Logística con SMOTE y evalúa en el conjunto de prueba.\n",
        "\n",
        "  Pasos:\n",
        "  1. Aplica SMOTE para balancear clases en el conjunto de entrenamiento.\n",
        "  2. Entrena un modelo.\n",
        "  3. Realiza predicciones en el conjunto de prueba.\n",
        "  4. Muestra matriz de confusión y reporte de clasificación.\n",
        "\n",
        "  Parámetros:\n",
        "  - X_train_VF: características del conjunto de entrenamiento (ya transformadas).\n",
        "  - y_train: etiquetas del conjunto de entrenamiento.\n",
        "  - X_test_VF: características del conjunto de prueba (ya transformadas).\n",
        "  - y_test: etiquetas del conjunto de prueba.\n",
        "  - random_state: semilla para reproducibilidad.\n",
        "\n",
        "  Retorna:\n",
        "  - model: modelo entrenado.\n",
        "  - y_pred: predicciones en el conjunto de prueba.\n",
        "  \"\"\"\n",
        "\n",
        "  smote = SMOTE(random_state=42)\n",
        "  X_train_resampled, y_train_resampled = smote.fit_resample(X_train_VF, y_train)\n",
        "\n",
        "  model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "  y_pred = model.predict(X_test_VF)\n",
        "\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "              xticklabels=['Pred 0', 'Pred 1'],\n",
        "              yticklabels=['Real 0', 'Real 1'])\n",
        "  plt.title('Matriz de Confusión')\n",
        "  plt.show()\n",
        "\n",
        "  print(\"\\nReporte de clasificación - Regresión Logística:\")\n",
        "  print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "  return model, y_pred\n"
      ],
      "metadata": {
        "id": "3OhDWIJmBy7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Px5SmNn-O5sM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}